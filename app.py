import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from io import BytesIO
from PIL import Image
import os
import json
import hashlib
import zipfile
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed


st.set_page_config(
    page_title="æ¼«ç”»ç”»åƒæŠ½å‡ºãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ–¼ï¸",
    layout="wide",
)

st.title("ğŸ–¼ï¸ æ¼«ç”»ç”»åƒæŠ½å‡ºãƒ„ãƒ¼ãƒ«ï¼ˆæŠ½å‡ºã ã‘ï¼‰")
st.markdown("URLã‹ã‚‰æ¼«ç”»ç”»åƒã‚’æŠ½å‡ºã—ã€ä¸€è¦§è¡¨ç¤ºãƒ»ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼ˆAIè§£æã¯ã—ã¾ã›ã‚“ï¼‰ã€‚")


def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _get_output_base_dir() -> str:
    """ä¿å­˜å…ˆï¼ˆãƒªãƒã‚¸ãƒˆãƒªå†… output/ï¼‰"""
    try:
        base = os.path.dirname(__file__)
    except Exception:
        base = os.getcwd()
    return os.path.join(base, "output")


def _ensure_output_dir() -> str:
    base = _get_output_base_dir()
    os.makedirs(base, exist_ok=True)
    return base


def _make_run_id() -> str:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    rnd = hashlib.sha256(os.urandom(16)).hexdigest()[:8]
    return f"{ts}_{rnd}"


def _zip_bytes_from_files(file_map: dict[str, bytes]) -> bytes:
    """{zipå†…ãƒ‘ã‚¹: bytes} ã‚’ZIPåŒ–ã—ã¦è¿”ã™"""
    buf = BytesIO()
    with zipfile.ZipFile(buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        for rel_path, data in file_map.items():
            zf.writestr(rel_path, data)
    return buf.getvalue()


def get_request_headers(url: str) -> dict:
    parsed_url = urlparse(url)
    base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
    return {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": base_domain,
    }


def get_pagination_urls(url: str, soup: BeautifulSoup, debug: bool = False) -> list[str]:
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®URLã‚’å–å¾—ï¼ˆåŒä¸€è¨˜äº‹å†…ã® /2 /3... ã‚’æƒ³å®šï¼‰"""
    urls = [url]

    pagination_selectors = [
        ".pagination a",
        ".page-numbers a",
        ".pager a",
        ".wp-pagenavi a",
        "nav.navigation a",
        ".post-page-numbers",
        "a.page-link",
        ".pages a",
    ]

    pagination_links = []
    for selector in pagination_selectors:
        links = soup.select(selector)
        if links:
            pagination_links.extend(links)
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º: {selector} ({len(links)}ä»¶)")
            break

    if not pagination_links:
        # rel=nextï¼ˆåŒä¸€è¨˜äº‹ã®æ¬¡ãƒšãƒ¼ã‚¸ã‚’æŒ‡ã™ã“ã¨ãŒå¤šã„ï¼‰
        rel_next = soup.select_one('a[rel="next"], link[rel="next"]')
        if rel_next and rel_next.get("href"):
            href = rel_next.get("href")
            # å¾Œæ®µã®å…±é€šå‡¦ç†ï¼ˆhrefã‚’èª­ã‚“ã§URLåŒ–ï¼‰ã«ä¹—ã›ã‚‹ãŸã‚ã€æ“¬ä¼¼çš„ã«aã¨ã—ã¦æ‰±ã†
            rel_next.name = "a"
            rel_next["href"] = href
            pagination_links.append(rel_next)
            if debug:
                st.write(f"rel=next ã‚’ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å€™è£œã¨ã—ã¦è¿½åŠ : {urljoin(url, href)}")

    if not pagination_links:
        all_links = soup.find_all("a")
        base_path = urlparse(url).path.rstrip("/")
        for link in all_links:
            text = link.get_text(strip=True)
            href = link.get("href", "")
            if not href:
                continue
            if text.isdigit():
                full_href = urljoin(url, href)
                href_path = urlparse(full_href).path.rstrip("/")
                if href_path.startswith(base_path):
                    pagination_links.append(link)
                    if debug:
                        st.write(f"æ•°å­—ãƒªãƒ³ã‚¯æ¤œå‡º: {text} -> {full_href}")

    if not pagination_links:
        # ã€Œæ¬¡ã®ãƒšãƒ¼ã‚¸ã€ç­‰ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªãƒ³ã‚¯ï¼ˆæ•°å­—ãƒªãƒ³ã‚¯ãŒç„¡ã„ã‚µã‚¤ãƒˆå‘ã‘ï¼‰
        for link in soup.find_all("a"):
            text = link.get_text(" ", strip=True)
            href = link.get("href", "")
            if not href or not text:
                continue
            if any(k in text for k in ["æ¬¡ã®ãƒšãƒ¼ã‚¸", "æ¬¡ãƒšãƒ¼ã‚¸", "next page", "Next Page"]):
                full_href = urljoin(url, href)
                if urlparse(full_href).netloc == urlparse(url).netloc:
                    pagination_links.append(link)
                    if debug:
                        st.write(f"æ¬¡ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆãƒªãƒ³ã‚¯æ¤œå‡º: {text} -> {full_href}")
                break

    base_path = urlparse(url).path.rstrip("/")
    seen = {url}
    for link in pagination_links:
        href = link.get("href")
        if not href:
            continue
        full_url = urljoin(url, href)
        if urlparse(full_url).netloc != urlparse(url).netloc:
            continue
        if full_url in seen:
            continue
        # åŒä¸€è¨˜äº‹ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã ã‘ã«é™å®šï¼ˆæ¬¡è©±/æ¬¡è¨˜äº‹ãƒŠãƒ“ãŒæ··ã–ã‚‹ã®ã‚’é˜²ãï¼‰
        full_path = urlparse(full_url).path.rstrip("/")
        if full_path != base_path and not _looks_like_intra_post_pagination(url, full_url):
            continue

        text = link.get_text(strip=True).lower()
        if text in ["next", "prev", "previous", "Â»", "Â«", "â€º", "â€¹", "æ¬¡ã¸", "å‰ã¸"]:
            continue
        urls.append(full_url)
        seen.add(full_url)

    def extract_page_num(u: str) -> int:
        path = urlparse(u).path.rstrip("/")
        if path == base_path:
            return 1
        if path.startswith(base_path + "/"):
            suffix = path[len(base_path) + 1 :]
            if suffix.isdigit():
                return int(suffix)
        return 999

    urls.sort(key=extract_page_num)

    if debug and len(urls) > 1:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸: {len(urls)}ãƒšãƒ¼ã‚¸")
        for u in urls:
            st.write(f"  - {u}")

    return urls


def get_page_images(url: str, debug: bool = False) -> tuple[list[dict], BeautifulSoup | None]:
    """ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º"""
    headers = get_request_headers(url)

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
    except requests.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return [], None

    soup = BeautifulSoup(response.content, "html.parser")
    images: list[dict] = []

    if debug:
        st.write(f"HTMLã‚µã‚¤ã‚º: {len(response.content)} bytes")

    content_selectors = [
        "article",
        ".entry-content",
        ".post-content",
        ".article-content",
        ".content",
        ".single-content",
        ".post-body",
        ".article-body",
        "main",
        "#content",
        "#main",
        ".post",
        ".entry",
        ".ystd",
        "#ystd",
    ]

    content_area = None
    for selector in content_selectors:
        content_area = soup.select_one(selector)
        if content_area:
            if debug:
                st.write(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢æ¤œå‡º: {selector}")
            break

    if not content_area:
        content_area = soup.body if soup.body else soup
        if debug:
            st.write("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢: bodyå…¨ä½“")

    img_tags = content_area.find_all("img")
    if debug:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸimgã‚¿ã‚°æ•°: {len(img_tags)}")

    skip_patterns = [
        "icon",
        "logo",
        "avatar",
        "emoji",
        "button",
        "banner",
        "advertisement",
        "widget",
        "gravatar",
        "favicon",
        "sprite",
        "pixel",
        "tracking",
        "analytics",
        "1x1",
    ]

    for img in img_tags:
        src = (
            img.get("src")
            or img.get("data-src")
            or img.get("data-lazy-src")
            or img.get("data-original")
            or img.get("data-full-url")
            or img.get("data-lazy")
            or img.get("data-image")
            or (img.get("data-srcset", "").split()[0] if img.get("data-srcset") else None)
            or (img.get("data-lazy-srcset", "").split()[0] if img.get("data-lazy-srcset") else None)
            or (img.get("srcset", "").split()[0] if img.get("srcset") else None)
        )
        if not src:
            if debug:
                st.write(f"âš ï¸ srcç„¡ã—: {str(img)[:100]}...")
            continue
        if src.startswith("data:"):
            if debug:
                st.write("âš ï¸ data URI ã‚¹ã‚­ãƒƒãƒ—")
            continue

        img_url = urljoin(url, src)
        if any(p in img_url.lower() for p in skip_patterns):
            if debug:
                st.write(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³: {img_url[:80]}...")
            continue

        img_extensions = [".jpg", ".jpeg", ".png", ".gif", ".webp", ".avif"]
        has_img_ext = any(ext in img_url.lower() for ext in img_extensions)

        img_path_patterns = ["/uploads/", "/images/", "/wp-content/", "/img/", "/photo/", "/manga/", "/comic/"]
        has_img_path = any(p in img_url.lower() for p in img_path_patterns)

        has_size_param = any(x in img_url.lower() for x in ["width=", "height=", "w=", "h=", "size=", "resize"])

        if has_img_ext or has_img_path or has_size_param:
            images.append({"url": img_url, "alt": img.get("alt", "")})
            if debug:
                st.write(f"âœ… ç”»åƒè¿½åŠ : {img_url[:80]}...")
        else:
            if debug:
                st.write(f"âŒ æ¡ä»¶ä¸ä¸€è‡´ã§ã‚¹ã‚­ãƒƒãƒ—: {img_url[:80]}...")

    # é‡è¤‡é™¤å»
    seen_urls: set[str] = set()
    unique_images: list[dict] = []
    for item in images:
        u = item.get("url", "")
        if not u or u in seen_urls:
            continue
        seen_urls.add(u)
        unique_images.append(item)

    return unique_images, soup


def _looks_like_intra_post_pagination(current_url: str, candidate_url: str) -> bool:
    """åŒä¸€è¨˜äº‹å†…ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ/2 /3 ...ï¼‰ã£ã½ã„URLã‹ã©ã†ã‹ã€‚

    ä¾‹:
    - current: https://aikatu.jp/archives/1031854
      cand:    https://aikatu.jp/archives/1031854/2
    - current: https://w.grapps.me/original/624941/
      cand:    https://w.grapps.me/original/624941/2/
    """
    try:
        cu = urlparse(current_url)
        nu = urlparse(candidate_url)
        if cu.netloc != nu.netloc:
            return False
        base = cu.path.rstrip("/")
        cand = nu.path.rstrip("/")
        if not cand.startswith(base + "/"):
            return False
        suffix = cand[len(base) + 1 :]
        return suffix.isdigit()
    except Exception:
        return False


def get_next_episode_url(soup: BeautifulSoup, base_url: str, debug: bool = False) -> str | None:
    """ã€Œæ¬¡ã®è©±>>ã€ã®URLã‚’å–å¾—ï¼ˆç‰¹å®šã‚µã‚¤ãƒˆå‘ã‘ã®ç·©ã„å®Ÿè£…ï¼‰"""
    # 1) æ—§ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç‰¹å®šã‚µã‚¤ãƒˆå‘ã‘ï¼‰
    next_episode_div = soup.find("div", class_="page-text-body", string=lambda t: t and "æ¬¡ã®è©±" in t)
    if next_episode_div:
        parent = next_episode_div.find_parent("a")
        if parent and parent.get("href"):
            next_url = urljoin(base_url, parent["href"])
            # /2ãªã©åŒä¸€è¨˜äº‹å†…ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Œæ¬¡è©±ã€ã§ã¯ãªã„
            if not _looks_like_intra_post_pagination(base_url, next_url):
                if debug:
                    st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º(div): {next_url}")
                return next_url
        next_link = next_episode_div.find_next("a")
        if next_link and next_link.get("href"):
            next_url = urljoin(base_url, next_link["href"])
            if not _looks_like_intra_post_pagination(base_url, next_url):
                if debug:
                    st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º(div-next): {next_url}")
                return next_url

    # 2) WordPressç³»ã®ã€Œæ¬¡ã®è¨˜äº‹ã€ãƒŠãƒ“ï¼ˆnav-nextï¼‰
    for sel in [
        "nav.post-navigation .nav-next a",
        "nav.navigation.post-navigation .nav-next a",
        ".post-navigation .nav-next a",
        ".navigation.post-navigation .nav-next a",
    ]:
        a = soup.select_one(sel)
        if a and a.get("href"):
            next_url = urljoin(base_url, a["href"])
            if _looks_like_intra_post_pagination(base_url, next_url):
                continue
            if debug:
                st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º(nav-next): {next_url}")
            return next_url

    # 3) ãƒ†ã‚­ã‚¹ãƒˆã§ã€Œæ¬¡ã®è©±ã€ã‚’å„ªå…ˆã—ã¦æ¢ã™ï¼ˆã€Œæ¬¡ã®ãƒšãƒ¼ã‚¸ã€ã‚ˆã‚Šå„ªå…ˆï¼‰
    keywords_strong = ["æ¬¡ã®è©±", "æ¬¡ã®è©±ï¼ï¼", "æ¬¡ã®è©±>>", "æ¬¡è©±", "æ¬¡ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"]
    for a in soup.find_all("a"):
        tx = a.get_text(" ", strip=True)
        href = a.get("href")
        if not tx or not href:
            continue
        if any(k in tx for k in keywords_strong):
            next_url = urljoin(base_url, href)
            if _looks_like_intra_post_pagination(base_url, next_url):
                continue
            if debug:
                st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º(text): {tx[:40]} -> {next_url}")
            return next_url

    if debug:
        st.write("â„¹ï¸ ã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    return None


def get_episode_images(url: str, episode_num: int = 1, debug: bool = False) -> tuple[list[dict], str | None]:
    """1è©±åˆ†ã®ç”»åƒã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¾¼ã¿ï¼‰"""
    first_page_images, soup = get_page_images(url, debug)
    if not soup:
        return [], None

    next_episode_url = get_next_episode_url(soup, url, debug)
    page_urls = get_pagination_urls(url, soup, debug)

    all_images: list[dict] = []
    seen_urls: set[str] = set()

    if debug:
        st.write(f"ğŸ“– ç¬¬{episode_num}è©±ã®å–å¾—é–‹å§‹")

    for img in first_page_images:
        if img["url"] in seen_urls:
            continue
        img["page"] = 1
        img["episode"] = episode_num
        all_images.append(img)
        seen_urls.add(img["url"])

    if len(page_urls) > 1:
        for i, page_url in enumerate(page_urls[1:], start=2):
            if debug:
                st.write(f"  ãƒšãƒ¼ã‚¸ {i} ã‚’å–å¾—ä¸­: {page_url}")
            page_images, page_soup = get_page_images(page_url, debug)
            for img in page_images:
                if img["url"] in seen_urls:
                    continue
                img["page"] = i
                img["episode"] = episode_num
                all_images.append(img)
                seen_urls.add(img["url"])
            if page_soup and not next_episode_url:
                next_episode_url = get_next_episode_url(page_soup, page_url, debug)

    if debug:
        st.write(f"ğŸ“– ç¬¬{episode_num}è©±: {len(all_images)}æšã®ç”»åƒã‚’å–å¾—")

    return all_images, next_episode_url


def get_multiple_episodes_images(url: str, num_episodes: int, debug: bool = False) -> list[dict]:
    """è¤‡æ•°è©±ã®ç”»åƒã‚’å–å¾—ï¼ˆæ¬¡ã®è©±ãƒªãƒ³ã‚¯ã‚’è¾¿ã‚‹ï¼‰"""
    all_images: list[dict] = []
    current_url: str | None = url

    for episode in range(1, num_episodes + 1):
        if not current_url:
            if debug:
                st.write(f"âš ï¸ ç¬¬{episode}è©±ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        if debug:
            st.write(f"ğŸ“š ç¬¬{episode}è©±ã‚’å–å¾—ä¸­: {current_url}")
        episode_images, next_url = get_episode_images(current_url, episode_num=episode, debug=debug)
        all_images.extend(episode_images)
        current_url = next_url
        if not next_url and episode < num_episodes:
            if debug:
                st.write(f"â„¹ï¸ ç¬¬{episode}è©±ãŒæœ€çµ‚è©±ã§ã™ã€‚{episode}è©±åˆ†ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
            break

    if debug:
        st.write(f"âœ… åˆè¨ˆ {len(all_images)}æšã®ç”»åƒã‚’å–å¾—")

    return all_images


def download_image(url: str, referer: str = "") -> bytes | None:
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Referer": referer,
    }
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.content
    except requests.RequestException:
        return None


@st.cache_data(show_spinner=False, ttl=60 * 10)
def _cached_download_image(url: str, referer: str = "") -> bytes | None:
    return download_image(url, referer)


def _download_and_validate_image(
    img_info: dict,
    min_size: int,
    referer: str,
) -> dict | None:
    """1æšã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
    img_data = download_image(img_info["url"], referer)
    if not img_data:
        return None

    if len(img_data) < min_size:
        return None

    try:
        img = Image.open(BytesIO(img_data))
        width, height = img.size
        aspect_ratio = width / height if height > 0 else 0

        if aspect_ratio > 3:
            return None
        if width < 200 or height < 200:
            return None

        return {
            **img_info,
            "data": img_data,
            "width": width,
            "height": height,
            "size": len(img_data),
        }
    except Exception:
        return None


def filter_manga_images(
    images: list[dict],
    min_size: int = 50_000,
    referer: str = "",
    debug: bool = False,
    max_workers: int = 10,
    progress_callback=None,
) -> list[dict]:
    """æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚µã‚¤ã‚º/ç¸¦æ¨ª/ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰- ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ"""
    manga_images: list[dict] = []
    total = len(images)
    completed = 0

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_img = {
            executor.submit(_download_and_validate_image, img_info, min_size, referer): img_info
            for img_info in images
        }

        results_map: dict[str, dict] = {}

        for future in as_completed(future_to_img):
            img_info = future_to_img[future]
            completed += 1

            if progress_callback:
                progress_callback(completed, total)

            try:
                result = future.result()
                if result:
                    results_map[img_info["url"]] = result
                    if debug:
                        st.write(f"âœ… å–å¾—æˆåŠŸ: {img_info['url'][:60]}...")
                else:
                    if debug:
                        st.write(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤–: {img_info['url'][:60]}...")
            except Exception as e:
                if debug:
                    st.write(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {img_info['url'][:60]}... - {e}")

    for img_info in images:
        if img_info["url"] in results_map:
            manga_images.append(results_map[img_info["url"]])

    return manga_images


def _guess_ext(img_bytes: bytes, fallback_ext: str = ".jpg") -> str:
    try:
        img = Image.open(BytesIO(img_bytes))
        fmt = (img.format or "").upper()
        if fmt == "JPEG":
            return ".jpg"
        if fmt == "PNG":
            return ".png"
        if fmt == "WEBP":
            return ".webp"
        if fmt == "GIF":
            return ".gif"
        if fmt == "AVIF":
            return ".avif"
    except Exception:
        pass
    return fallback_ext


def build_images_zip(manga_images: list[dict]) -> tuple[bytes, dict[str, str]]:
    """ç”»åƒã‚’ZIPåŒ–ã—ã¦è¿”ã™ã€‚æˆ»ã‚Šå€¤ã¯(zip_bytes, filename_map[url]=zipå†…ãƒ‘ã‚¹)"""
    file_map: dict[str, bytes] = {}
    name_map: dict[str, str] = {}

    for idx, img in enumerate(manga_images, start=1):
        ep = int(img.get("episode", 1) or 1)
        page = int(img.get("page", 1) or 1)
        ext = _guess_ext(img.get("data") or b"")
        rel = f"images/ep{ep:02d}_p{page:03d}_{idx:04d}{ext}"
        file_map[rel] = img["data"]
        name_map[img.get("url", f"idx:{idx}")] = rel

    zip_bytes = _zip_bytes_from_files(file_map)
    return zip_bytes, name_map


with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=False, help="ç”»åƒæ¤œå‡ºã®è©³ç´°ã‚’è¡¨ç¤ºã—ã¾ã™")
    min_image_size_kb = st.slider(
        "æœ€å°ç”»åƒã‚µã‚¤ã‚º (KB)",
        min_value=1,
        max_value=800,
        value=30,
        help="ã“ã®å€¤ã‚ˆã‚Šå°ã•ã„ç”»åƒã¯é™¤å¤–ã•ã‚Œã¾ã™",
    )
    max_images_total = st.slider(
        "æŠ½å‡ºã™ã‚‹æœ€å¤§ç”»åƒæšæ•°ï¼ˆä¸Šé™ï¼‰",
        min_value=5,
        max_value=300,
        value=120,
        step=5,
        help="å¤šã„ã»ã©é‡ããªã‚Šã¾ã™ï¼ˆè¡¨ç¤º/ZIPã‚‚å¤§ãããªã‚Šã¾ã™ï¼‰",
    )
    parallel_downloads = st.slider(
        "ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°",
        min_value=1,
        max_value=20,
        value=10,
        help="åŒæ™‚ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç”»åƒæ•°ã€‚å¤§ãã„ã»ã©é€Ÿã„ã§ã™ãŒã‚µãƒ¼ãƒãƒ¼è² è·ãŒä¸ŠãŒã‚Šã¾ã™",
    )
    st.divider()
    st.subheader("ğŸ“š å–å¾—ç¯„å›²")
    mode = st.radio(
        "å–å¾—ãƒ¢ãƒ¼ãƒ‰",
        options=["ã‚¨ãƒ”æ¼«ç”»ï¼ˆ1è©±ï¼‰", "é€£è¼‰æ¼«ç”»ï¼ˆ3è©±ï¼‰", "é€£è¼‰æ¼«ç”»ï¼ˆ10è©±ï¼‰", "ä»»æ„è©±æ•°"],
        index=0,
    )
    if mode == "ä»»æ„è©±æ•°":
        num_episodes = st.number_input("è©±æ•°", min_value=1, max_value=30, value=1, step=1)
    elif mode == "é€£è¼‰æ¼«ç”»ï¼ˆ10è©±ï¼‰":
        num_episodes = 10
    elif mode == "é€£è¼‰æ¼«ç”»ï¼ˆ3è©±ï¼‰":
        num_episodes = 3
    else:
        num_episodes = 1


url = st.text_input(
    "æ¼«ç”»è¨˜äº‹URL",
    placeholder="https://example.com/manga/xxxx",
    help="æ¼«ç”»ãƒšãƒ¼ã‚¸ï¼ˆé–‹å§‹è©±ï¼‰ã®URLã‚’å…¥ã‚Œã¦ãã ã•ã„",
)

col1, col2 = st.columns([1, 4])
with col1:
    extract_button = st.button("ğŸ–¼ï¸ æŠ½å‡ºé–‹å§‹", type="primary", use_container_width=True)


if extract_button:
    if not url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—ä¸­..."):
            images = get_multiple_episodes_images(url, num_episodes=int(num_episodes), debug=debug_mode)

        if not images:
            st.warning("ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’ONã«ã—ã¦è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ“· {len(images)}ä»¶ã®ç”»åƒå€™è£œã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ï¼ˆ{parallel_downloads}ä¸¦åˆ—ï¼‰...")

            progress_bar = st.progress(0, text="ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

            def update_progress(completed: int, total: int):
                progress = completed / total
                progress_bar.progress(progress, text=f"ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... {completed}/{total}")

            manga_images = filter_manga_images(
                images,
                min_size=int(min_image_size_kb) * 1000,
                referer=url,
                debug=debug_mode,
                max_workers=int(parallel_downloads),
                progress_callback=update_progress,
            )

            progress_bar.empty()

            if not manga_images:
                st.warning("æ¼«ç”»ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šï¼ˆæœ€å°ã‚µã‚¤ã‚ºãªã©ï¼‰ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
                if debug_mode and images:
                    st.subheader("æ¤œå‡ºã•ã‚ŒãŸç”»åƒURLä¸€è¦§ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å‰ï¼‰")
                    for img in images:
                        st.text(img["url"])
            else:
                if len(manga_images) > int(max_images_total):
                    st.warning(f"âš ï¸ ç”»åƒãŒ{len(manga_images)}æšã‚ã‚Šã¾ã™ã€‚ä¸Šé™ã«ã‚ˆã‚Šå…ˆé ­{int(max_images_total)}æšã ã‘æ‰±ã„ã¾ã™ã€‚")
                    manga_images = manga_images[: int(max_images_total)]

                # è©±æ•°ã”ã¨ã®æšæ•°
                episode_counts: dict[int, int] = {}
                for img in manga_images:
                    ep = int(img.get("episode", 1) or 1)
                    episode_counts[ep] = episode_counts.get(ep, 0) + 1
                episode_summary = "ã€".join([f"ç¬¬{ep}è©±: {count}æš" for ep, count in sorted(episode_counts.items())])
                st.success(f"âœ… {len(manga_images)}ä»¶ã®æ¼«ç”»ç”»åƒã‚’æŠ½å‡ºã—ã¾ã—ãŸï¼ˆ{episode_summary}ï¼‰")

                st.divider()
                st.subheader("ğŸ–¼ï¸ æŠ½å‡ºçµæœï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")

                cols_per_row = 3
                for i in range(0, len(manga_images), cols_per_row):
                    cols = st.columns(cols_per_row)
                    for j, col in enumerate(cols):
                        idx = i + j
                        if idx >= len(manga_images):
                            continue
                        img_info = manga_images[idx]
                        with col:
                            ep = int(img_info.get("episode", 1) or 1)
                            page = int(img_info.get("page", 1) or 1)
                            st.image(
                                img_info["data"],
                                caption=f"ç¬¬{ep}è©± P{page} / {img_info.get('width')}x{img_info.get('height')} / {int(img_info.get('size',0))/1024:.1f}KB",
                                use_container_width=True,
                            )

                st.divider()
                st.subheader("â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

                zip_bytes, name_map = build_images_zip(manga_images)
                run_id = _make_run_id()
                st.download_button(
                    "ç”»åƒZIPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=zip_bytes,
                    file_name=f"manga_images_{run_id}.zip",
                    mime="application/zip",
                    use_container_width=True,
                )

                # JSONï¼ˆURLã¨ãƒ¡ã‚¿ï¼‰
                items = []
                for img in manga_images:
                    items.append(
                        {
                            "episode": int(img.get("episode", 1) or 1),
                            "page": int(img.get("page", 1) or 1),
                            "url": img.get("url", ""),
                            "alt": img.get("alt", ""),
                            "width": int(img.get("width", 0) or 0),
                            "height": int(img.get("height", 0) or 0),
                            "size_bytes": int(img.get("size", 0) or 0),
                            "zip_path": name_map.get(img.get("url", ""), ""),
                        }
                    )

                st.download_button(
                    "ç”»åƒä¸€è¦§JSONã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json.dumps(items, ensure_ascii=False, indent=2).encode("utf-8"),
                    file_name=f"manga_images_{run_id}.json",
                    mime="application/json",
                    use_container_width=True,
                )

                with st.expander("ğŸ’¾ output/ ã«ä¿å­˜ï¼ˆä»»æ„ï¼‰", expanded=False):
                    st.caption("ã‚µãƒ¼ãƒãƒ¼ä¸Šã® `output/<run_id>/` ã«ä¿å­˜ã—ã¾ã™ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é‹ç”¨å‘ã‘ï¼‰ã€‚")
                    if st.button("ä¿å­˜ã™ã‚‹", use_container_width=True):
                        base = _ensure_output_dir()
                        run_dir = os.path.join(base, run_id)
                        img_dir = os.path.join(run_dir, "images")
                        os.makedirs(img_dir, exist_ok=True)

                        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                        for img in manga_images:
                            zp = name_map.get(img.get("url", ""), "")
                            if not zp.startswith("images/"):
                                continue
                            rel_name = zp[len("images/") :]
                            out_path = os.path.join(img_dir, rel_name)
                            with open(out_path, "wb") as f:
                                f.write(img["data"])

                        meta = {
                            "url": url,
                            "num_episodes": int(num_episodes),
                            "min_image_size_kb": int(min_image_size_kb),
                            "max_images_total": int(max_images_total),
                            "total_candidates": len(images),
                            "total_extracted": len(manga_images),
                            "episode_counts": episode_counts,
                        }
                        with open(os.path.join(run_dir, "images.json"), "w", encoding="utf-8") as f:
                            json.dump(items, f, ensure_ascii=False, indent=2)
                        with open(os.path.join(run_dir, "meta.json"), "w", encoding="utf-8") as f:
                            json.dump(meta, f, ensure_ascii=False, indent=2)

                        st.success(f"ä¿å­˜ã—ã¾ã—ãŸ: output/{run_id}/")

                if debug_mode:
                    st.divider()
                    st.subheader("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
                    st.write("å€™è£œç”»åƒï¼ˆãƒ•ã‚£ãƒ«ã‚¿å‰ï¼‰:", len(images))
                    st.write("æŠ½å‡ºç”»åƒï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰:", len(manga_images))
                    st.write("å…¥åŠ›URLã®ãƒ‰ãƒ¡ã‚¤ãƒ³:", urlparse(url).netloc)
                    st.write("URLã®ãƒãƒƒã‚·ãƒ¥:", _sha256_text(url)[:16])


